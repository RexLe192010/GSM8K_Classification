今天在公司里将我自己的分类跑起来了，但是第一次的代码并没有加入多进程，训练集7000个sample要完整跑下来确实还是要花很多时间的。后来我让chatgpt帮我在我的代码中加入了多进程的代码，我也打算后续再把训练集的量减少到1000左右，这样时间会减短一些。

有关实习初期的in-context executing，何老师指出由于传入chatgpt的内容是一段完整的对话，很容易撑破字数上限，因此这个项目可能得要终止；接下去的项目的主要内容是当接到任务时进行一个分类，选择直接作答还是chain-of-thought，进行多次分类之后做出正确率最高的优质回答。我今天写的代码内容就是有关这个项目的，因此，不会再放到20230620的文件夹中。

接下来一段时间我需要等待运行的结果，在这期间可以把机器学习的速通视频看完。


分类的评估方法：

精确率和召回率---引入混淆矩阵的概念
在分类任务下，预测结果与正确标记之间存在四种不同的组合，构成混淆矩阵（适用于多分类）
	正例	假例
正例	TP	FN
假例	FP	TN
在这当中TP是True Positive，FN是False Negative

精确率（Precision）：TP/(TP + FP)；召回率（Recall）：TP/(TP + FN)
如果以癌症作为例子，那么召回率就是患癌症的病人能被查出来的概率；而精确率则是在检测出癌症的病人中，真正患有癌症的比例
还有另一个评估标准：F1-score
F1 = 2TP/(2TP + FN + FP)
应用场景：癌症检查，工厂里的质量检测（检查次品）等

分类评估的API：sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None)
y_true是真实目标值，y_pred是估计器预测目标值，labels是指定类别对应的数字，target_names是目标类别名称
用report = classification_report(...)来调用

问题：如何衡量样本不均衡下的评估？---ROC曲线和AUC指标
TPR和FPR：TPR = TP/(TP + FN)所有真实类别为1的样本中，预测类别为1的比例；
          FPR = FP/(FP + TN)所有真实类别为0的样本中，预测类别为1的比例

ROC曲线：FPR作为横坐标，TPR作为纵坐标得出来的曲线
当TPR = FPR的时候，相当于不管怎么样都预测类别为1，即瞎猜（50%蒙对），AUC = 0.5
当AUC > 0.5，说明在真实类别为1的情况下，预测为1，优于随机猜测
当AUC = 1，称之为完美分类器

API：sklearn.metrics.roc_auc_score(y_true, y_score)
用来计算ROC曲线下方面积，即AUC；
y_true：每个样本的真实类别，必须为0（反例），1（正例）标记
y_score：预测得分，可以是正类的估计概率，置信值或者分类器方法的返回值
